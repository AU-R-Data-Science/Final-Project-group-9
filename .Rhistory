library(boot)
library(mlbench)
data("PimaIndiansDiabetes")
data=read.csv("winequality-red.csv")
data=PimaIndiansDiabetes
data=data[data$class=="Iris-setosa" | data$class=="Iris-virginica",]
library(caret)
library(boot)
library(mlbench)
data("PimaIndiansDiabetes")
data=read.csv("winequality-red.csv")
data=PimaIndiansDiabetes
data=data[data$class=="Iris-setosa" | data$class=="Iris-virginica",]
data$class=ifelse(data$class=="Iris-setosa",1,0)
data$quality=ifelse(data$quality<6,1,0)
n_features=ncol(data)
dt = sort(sample(nrow(data), nrow(data)*.7))
train<-data[dt,]
test<-data[-dt,]
x=as.matrix(train[,1:n_features-1])
y=train[,n_features]
xtest=test[,1:n_features-1]
ytest=test[,n_features]
weightInitialization=function(x,y){
init_weights=solve(t(x)%*%x)%*%t(x)%*%y
init_intercept = mean(y) - (colMeans(x)%*%init_weights)
init=c(init_intercept,init_weights)
names(init)=c("intercept",colnames(x))
return(init)
}
init=weightInitialization(x,y)
library(caret)
library(boot)
head(data)
data=read.csv(file.choose())
head(data)
data=data[data$class=="Iris-setosa" | data$class=="Iris-virginica",]
data$class=ifelse(data$class==c("Iris-setosa"),1,0)
n_features=ncol(data)
dt = sort(sample(nrow(data), nrow(data)*.7))
train<-data[dt,]
test<-data[-dt,]
x=as.matrix(train[,1:n_features-1])
y=train[,n_features]
xtest=test[,1:n_features-1]
ytest=test[,n_features]
weightInitialization=function(x,y){
init_weights=solve(t(x)%*%x)%*%t(x)%*%y
init_intercept = mean(y) - (colMeans(x)%*%init_weights)
init=c(init_intercept,init_weights)
names(init)=c("intercept",colnames(x))
return(init)
}
weightInitialization(x,y)
sigmoid=function(result){
final_result = 1/(1+exp(-result))
return(final_result)
}
model_optimize=function(init, x, y){
m = nrow(x)
w=init[2:length(init)]
b=init[1]
#Prediction
final_result = sigmoid(w%*%t(x)+b)
Y_T = t(y)
cost = (-1/m)*(sum((log(final_result)*Y_T) + (log(1-final_result)*(1-Y_T))))
#Gradient calculation
dw = (1/m)*((t(x)%*%t((final_result-t(y)))))
db = (1/m)*(sum(final_result-t(y)))
grads = c(db,dw)
return(c(cost,grads))
}
model_predict=function(init, x, y, learning_rate=0.05, no_iterations=1500){
costs = c()
w=init[2:length(init)]
b=init[1]
for(i in 1:no_iterations){
optim = model_optimize(init,x,y)
grads = c(optim[2:length(optim)])
cost = optim[1]
dw = c(grads[2:length(grads)])
db = grads[1]
#weight update
w = w - (dw*learning_rate)
b = b - (db*learning_rate)
init=c(b,w)
costs=c(costs,cost)
}
#final parameters
coeff = c(b,w)
gradient = c(db,dw)
return(list(coeff,gradient,costs))
}
predictor=function(final_pred, m, cut_off=0.5){
y_pred = rep(0,m)
for(i in 1:length(final_pred)){
if(final_pred[i] > cut_off)
y_pred[i] = 1
else
y_pred[i] = 0
}
return(y_pred)
}
init= weightInitialization(x,y)
#Gradient Descent
predict=model_predict(init, x, y)
coeff=unlist(predict[1])
names(coeff)=c("intercept",colnames(x))
gradient=unlist(predict[2])
costs = unlist(predict[3])
w=coeff[2:length(init)]
b=coeff[1]
final_train_pred = sigmoid(w%*%t(x)+b)
final_test_pred = sigmoid(w%*%t(xtest)+b)
m_tr =  nrow(x)
m_ts =  nrow(xtest)
y_tr_pred = predictor(final_train_pred, m_tr,cut_off=0.5)
y_ts_pred = predictor(final_test_pred, m_ts)
coeff
min(costs)
plot(c(1:length(costs)),costs,type="l")
table(ytest)
table(y_ts_pred)
df=data.frame(y_ts_pred,ytest)
install.packages("rlang")
library(ggplot2)
library(lattice)
library(caret)
confusion_matrix=confusionMatrix(as.factor(df$ytest), as.factor(df$y_ts_pred))
confusion_matrix
ggplot(df, aes(x=ytest, y=final_test_pred)) + geom_point() +
stat_smooth(method="glm", color="green", se=FALSE,
method.args = list(family=binomial))
bootstrapCI=function(data,alpha,n=20){
n_features=ncol(data)
beta=matrix(nrow=n,ncol=n_features)
for(i in 1:n){
n_features=ncol(data)
dt = sort(sample(nrow(data), nrow(data)*.7))
train<-data[dt,]
x=as.matrix(train[,1:n_features-1])
y=train[,n_features]
init= weightInitialization(x,y)
predict=model_predict(init, x, y)
coeff=unlist(predict[1])
beta[i,]=coeff
}
CI=matrix(nrow=n_features,ncol=3)
for(i in 1:n_features){
b1<-boot(beta[i,],function(u,i) mean(u[i]),R=n)
g=boot.ci(b1,type=c("norm","basic","perc"),conf=1-alpha)$norm
CI[i,]=as.vector(g)
}
rownames(CI)=c("intercept",colnames(x))
colnames(CI)=c("Level","Lower","Upper")
return(CI)
}
CI=bootstrapCI(data,0.15,n=100)
CI
library(boot)
library(lattice)
x1<-rnorm(50,2,0.25)
b1<-boot(x1,function(u,i) mean(u[i]),R=1000)
CI=boot.ci(b1,type=c("norm"))
CI$norm
b1
n=20
CI
library(caret)
library(boot)
library(mlbench)
data("PimaIndiansDiabetes")
data=read.csv("winequality-red.csv")
data=PimaIndiansDiabetes
data=data[data$class=="Iris-setosa" | data$class=="Iris-virginica",]
data$class=ifelse(data$class=="Iris-setosa",1,0)
data$quality=ifelse(data$quality<6,1,0)
n_features=ncol(data)
dt = sort(sample(nrow(data), nrow(data)*.7))
train<-data[dt,]
test<-data[-dt,]
x=as.matrix(train[,1:n_features-1])
y=train[,n_features]
xtest=test[,1:n_features-1]
ytest=test[,n_features]
weightInitialization=function(x,y){
init_weights=solve(t(x)%*%x)%*%t(x)%*%y
init_intercept = mean(y) - (colMeans(x)%*%init_weights)
init=c(init_intercept,init_weights)
names(init)=c("intercept",colnames(x))
return(init)
}
init=weightInitialization(x,y)
#data("PimaIndiansDiabetes")
data=read.csv("winequality-red.csv")
#data("PimaIndiansDiabetes")
data=read.csv("iris_csv.csv")
#data=PimaIndiansDiabetes
#data=data[data$class=="Iris-setosa" | data$class=="Iris-virginica",]
#data$class=ifelse(data$class=="Iris-setosa",1,0)
data$quality=ifelse(data$quality<6,1,0)
#data("PimaIndiansDiabetes")
data=read.csv("iris_csv.csv")
#data=PimaIndiansDiabetes
data=data[data$class=="Iris-setosa" | data$class=="Iris-virginica",]
data$class=ifelse(data$class=="Iris-setosa",1,0)
#data$quality=ifelse(data$quality<6,1,0)
n_features=ncol(data)
dt = sort(sample(nrow(data), nrow(data)*.7))
train<-data[dt,]
test<-data[-dt,]
x=as.matrix(train[,1:n_features-1])
y=train[,n_features]
xtest=test[,1:n_features-1]
ytest=test[,n_features]
weightInitialization=function(x,y){
init_weights=solve(t(x)%*%x)%*%t(x)%*%y
init_intercept = mean(y) - (colMeans(x)%*%init_weights)
init=c(init_intercept,init_weights)
names(init)=c("intercept",colnames(x))
return(init)
}
init=weightInitialization(x,y)
sigmoid=function(result){
final_result = 1/(1+exp(-result))
return(final_result)
}
cost.glm <- function(theta,X) {
m <- nrow(X)
g <- sigmoid(X%*%theta)
(1/m)*sum((-y*log(g)) - ((1-y)*log(1-g)))
}
x1 <- cbind(1, x)
predict=optim(par=init, fn = cost.glm, method='CG',
X=x1)
coeff=unlist(predict[1])
names(coeff)=c("intercept",colnames(x))
cost=unlist(predict[2])
w=coeff[2:length(coeff)]
b=coeff[1]
final_train_pred = sigmoid(w%*%t(x)+b)
final_test_pred = sigmoid(w%*%t(xtest)+b)
m_tr =  nrow(x)
m_ts =  nrow(xtest)
predictor=function(final_pred, m, cutoff=0.5){
y_pred = rep(0,m)
for(i in 1:length(final_pred)){
if(final_pred[i] > cutoff)
y_pred[i] = 1
else
y_pred[i] = 0
}
return(y_pred)
}
y_tr_pred = predictor(final_train_pred, m_tr,cutoff=0.5)
y_ts_pred = predictor(final_test_pred, m_ts)
#plot(c(1:length(costs)),costs,type="l")
table(ytest)
table(y_ts_pred)
newdata <- data.frame(prob=seq(min(ytest), max(ytest),length=length(ytest)))
newdata$class=predictor(newdata$prob, m_ts)
plot(class~prob, data=newdata, col="steelblue")
lines(class~prob, newdata, lwd=2)
ggplot(newdata, aes(x=prob, y=class)) +
geom_point(alpha=.5) +
geom_smooth(method="glm", se=FALSE, method.args = list(family=binomial))
metricplot=function(y_prob,ytest){
m_tr =  nrow(y_prob)
metrics=matrix(nrow=9,ncol=7)
colnames(metrics)=c("Prevalence","Accuracy","Sensitivity","Specificity","False Discovery Rate","Diagnostic Odds Ratio","cutoff")
for(i in 1:9){
y_ts_pred = predictor(y_prob, m_tr,cutoff=i/10)
confusion_matrix=confusionMatrix(as.factor(ytest), as.factor(y_ts_pred))
metrics[i,1]=as.vector(confusion_matrix$byClass[8])
metrics[i,2]=as.vector(confusion_matrix$overall[1])
metrics[i,3]=as.vector(confusion_matrix$byClass[1])
metrics[i,4]=as.vector(confusion_matrix$byClass[2])
metrics[i,5]=1-as.vector(confusion_matrix$byClass[3])
metrics[i,6]=metrics[i,2]*metrics[i,3]/((1-metrics[i,2])*(1-metrics[i,3]))
metrics[i,7]=i/10
}
return(metrics)
}
metricplot(final_test_pred,ytest)
confusion_matrix
confusion_matrix=confusionMatrix(as.factor(ytest), as.factor(y_ts_pred))
confusion_matrix
metric_data=metricplot(final_test_pred,ytest)
metric_data
plot(y=metric_data$Prevalence,x=metric_data$cutoff)
metric_data$cutoff
metric_data
plot(y=metric_data[,1],x=metric_data[,7])
plot(y=metric_data[,1],x=metric_data[,7],type="l")
plot(y=metric_data[,"Prevalence"],x=metric_data[,7],type="l"))
plot(y=metric_data[,"Prevalence"],x=metric_data[,7],type="l")
metric="Prevalence"
metricplot=function(y_prob,ytest,metric){
m_tr =  nrow(y_prob)
metrics=matrix(nrow=9,ncol=7)
colnames(metrics)=c("Prevalence","Accuracy","Sensitivity","Specificity","False Discovery Rate","Diagnostic Odds Ratio","cutoff")
for(i in 1:9){
y_ts_pred = predictor(y_prob, m_tr,cutoff=i/10)
confusion_matrix=confusionMatrix(as.factor(ytest), as.factor(y_ts_pred))
metrics[i,1]=as.vector(confusion_matrix$byClass[8])
metrics[i,2]=as.vector(confusion_matrix$overall[1])
metrics[i,3]=as.vector(confusion_matrix$byClass[1])
metrics[i,4]=as.vector(confusion_matrix$byClass[2])
metrics[i,5]=1-as.vector(confusion_matrix$byClass[3])
metrics[i,6]=metrics[i,2]*metrics[i,3]/((1-metrics[i,2])*(1-metrics[i,3]))
metrics[i,7]=i/10
}
return(plot(y=metric_data[,metric],x=metric_data[,7],type="l"))
}
metricplot(final_test_pred,ytest,metric=metric)
metric="Prevalence"
metricplot=function(y_prob,ytest,metric){
m_tr =  nrow(y_prob)
metrics=matrix(nrow=9,ncol=7)
colnames(metrics)=c("Prevalence","Accuracy","Sensitivity","Specificity","False Discovery Rate","Diagnostic Odds Ratio","cutoff")
for(i in 1:9){
y_ts_pred = predictor(y_prob, m_tr,cutoff=i/10)
confusion_matrix=confusionMatrix(as.factor(ytest), as.factor(y_ts_pred))
metrics[i,1]=as.vector(confusion_matrix$byClass[8])
metrics[i,2]=as.vector(confusion_matrix$overall[1])
metrics[i,3]=as.vector(confusion_matrix$byClass[1])
metrics[i,4]=as.vector(confusion_matrix$byClass[2])
metrics[i,5]=1-as.vector(confusion_matrix$byClass[3])
metrics[i,6]=metrics[i,2]*metrics[i,3]/((1-metrics[i,2])*(1-metrics[i,3]))
metrics[i,7]=i/10
}
return(plot(y=metric_data[,metric],x=metric_data[,7],type="l"))
}
metricplot(final_test_pred,ytest,metric=metric)
metricplot(final_test_pred,ytest,metric="Accuracy")
metricplot(final_test_pred,ytest,metric="Sensitivity")
metricplot=function(y_prob,ytest,metric){
m_tr =  nrow(y_prob)
metrics=matrix(nrow=9,ncol=7)
colnames(metrics)=c("Prevalence","Accuracy","Sensitivity","Specificity","False Discovery Rate","Diagnostic Odds Ratio","cutoff")
for(i in 1:9){
y_ts_pred = predictor(y_prob, m_tr,cutoff=i/10)
confusion_matrix=confusionMatrix(as.factor(ytest), as.factor(y_ts_pred))
metrics[i,1]=as.vector(confusion_matrix$byClass[8])
metrics[i,2]=as.vector(confusion_matrix$overall[1])
metrics[i,3]=as.vector(confusion_matrix$byClass[1])
metrics[i,4]=as.vector(confusion_matrix$byClass[2])
metrics[i,5]=1-as.vector(confusion_matrix$byClass[3])
metrics[i,6]=metrics[i,2]*metrics[i,3]/((1-metrics[i,2])*(1-metrics[i,3]))
metrics[i,7]=i/10
}
return(plot(y=metric_data[,metric],x=metric_data[,7],type="l",xlab="cutoff",ylab=metric))
}
metricplot(final_test_pred,ytest,metric="Sensitivity")
metricplot=function(y_prob,ytest,metric){
m_tr =  nrow(y_prob)
metrics=matrix(nrow=9,ncol=7)
colnames(metrics)=c("Prevalence","Accuracy","Sensitivity","Specificity","False Discovery Rate","Diagnostic Odds Ratio","cutoff")
for(i in 1:9){
y_ts_pred = predictor(y_prob, m_tr,cutoff=i/10)
confusion_matrix=confusionMatrix(as.factor(ytest), as.factor(y_ts_pred))
metrics[i,1]=as.vector(confusion_matrix$byClass[8])
metrics[i,2]=as.vector(confusion_matrix$overall[1])
metrics[i,3]=as.vector(confusion_matrix$byClass[1])
metrics[i,4]=as.vector(confusion_matrix$byClass[2])
metrics[i,5]=1-as.vector(confusion_matrix$byClass[3])
metrics[i,6]=metrics[i,2]*metrics[i,3]/((1-metrics[i,2])*(1-metrics[i,3]))
metrics[i,7]=i/10
}
return(plot(y=metric_data[,metric],x=metric_data[,7],type="l",xlab="cutoff",ylab=metric))
}
metricplot(final_test_pred,ytest,metric="Specificity")
plot(class~prob, data=newdata, col="steelblue")
lines(class~prob, newdata, lwd=2)
newdata
final_test_pred
#plot(c(1:length(costs)),costs,type="l")
table(ytest)
table(y_ts_pred)
newdata <- data.frame(prob=seq(min(ytest), max(ytest),length=length(ytest)))
newdata$variable=predictor(xtest[1], m_ts)
newdata$variable=predictor(xtest[1,], m_ts)
plot(class~prob, data=newdata, col="steelblue")
#data("PimaIndiansDiabetes")
data=read.csv("iris_csv.csv")
data
#data=PimaIndiansDiabetes
data=data[data$class=="Iris-setosa" | data$class=="Iris-virginica",]
data$class=ifelse(data$class=="Iris-setosa",1,0)
data
dt = sort(sample(nrow(data), nrow(data)*.7))
train<-data[dt,]
test<-data[-dt,]
x=as.matrix(train[,1:n_features-1])
y=train[,n_features]
xtest=test[,1:n_features-1]
ytest=test[,n_features]
weightInitialization=function(x,y){
init_weights=solve(t(x)%*%x)%*%t(x)%*%y
init_intercept = mean(y) - (colMeans(x)%*%init_weights)
init=c(init_intercept,init_weights)
names(init)=c("intercept",colnames(x))
return(init)
}
init=weightInitialization(x,y)
init
sigmoid=function(result){
final_result = 1/(1+exp(-result))
return(final_result)
}
predict
w
b
final_test_pred
y_ts_pred
newdata
bootstrapCI=function(data,alpha,n=20){
n_features=ncol(data)
beta=matrix(nrow=n,ncol=n_features)
for(i in 1:n){
n_features=ncol(data)
dt = sort(sample(nrow(data), nrow(data)*.7))
train<-data[dt,]
x=as.matrix(train[,1:n_features-1])
y=train[,n_features]
init= weightInitialization(x,y)
x1 <- cbind(1, x)
predict=optim(par=init, fn = cost.glm, method='CG',
X=x1)
coeff=unlist(predict[1])
beta[i,]=coeff
}
CI=matrix(nrow=n_features,ncol=3)
for(i in 1:n_features){
b1<-boot(beta[i,],function(u,i) mean(u[i]),R=n)
g=boot.ci(b1,type=c("norm","basic","perc"),conf=1-alpha)$norm
CI[i,]=as.vector(g)
}
rownames(CI)=c("intercept",colnames(x))
colnames(CI)=c("Level","Lower","Upper")
return(CI)
}
CI=bootstrapCI(data,0.05)
CI
CI=bootstrapCI(data,n=30,0.1)
CI
confusion_matrix=confusionMatrix(as.factor(ytest), as.factor(y_ts_pred))
confusion_matrix
metricplot=function(y_prob,ytest,metric){
m_tr =  nrow(y_prob)
metrics=matrix(nrow=9,ncol=7)
colnames(metrics)=c("Prevalence","Accuracy","Sensitivity","Specificity","False Discovery Rate","Diagnostic Odds Ratio","cutoff")
for(i in 1:9){
y_ts_pred = predictor(y_prob, m_tr,cutoff=i/10)
confusion_matrix=confusionMatrix(as.factor(ytest), as.factor(y_ts_pred))
metrics[i,1]=as.vector(confusion_matrix$byClass[8])
metrics[i,2]=as.vector(confusion_matrix$overall[1])
metrics[i,3]=as.vector(confusion_matrix$byClass[1])
metrics[i,4]=as.vector(confusion_matrix$byClass[2])
metrics[i,5]=1-as.vector(confusion_matrix$byClass[3])
metrics[i,6]=metrics[i,2]*metrics[i,3]/((1-metrics[i,2])*(1-metrics[i,3]))
metrics[i,7]=i/10
}
return(plot(y=metric_data[,metric],x=metric_data[,7],type="l",xlab="cutoff",ylab=metric))
}
metricplot(final_test_pred,ytest,metric="Specificity")
metricplot(final_test_pred,ytest,metric="Accuracy")
newdata <- matrix(prob=seq(min(ytest), max(ytest),length=length(ytest)))
newdata <- matrix(seq(min(ytest), max(ytest),length=length(ytest)))
View(newdata)
newdata$variable=predictor(xtest[1,], m_ts)
View(newdata)
newdata$variable=as.vector(predictor(xtest[1,], m_ts))
newdata <- matrix(seq(min(ytest), max(ytest),length=length(ytest)))
newdata$variable=as.vector(predictor(xtest[1,], m_ts))
newdata$variable=as.numeric(predictor(xtest[1,], m_ts))
newdata <- matrix(seq(min(ytest), max(ytest),length=length(ytest)))
newdata$variable=as.numeric(predictor(xtest[1,], m_ts))
#plot(c(1:length(costs)),costs,type="l")
table(ytest)
table(y_ts_pred)
newdata <- matrix(seq(min(ytest), max(ytest),length=length(ytest)))
newdata$variable=xtest[1,]
newdata[,2]=xtest[1,]
xtest[1,]
newdata[,2]=xtest[,1]
newdata <- matrix(seq(min(ytest), max(ytest),length=length(ytest)))
newdata=cbind(newdata,xtest[,2])
View(newdata)
plot(V1~V2, data=newdata, col="steelblue")
lines(V1~V2, newdata, lwd=2)
newdata=sort(cbind(newdata,xtest[,2]))
plot(V1~V2, data=newdata, col="steelblue")
lines(V1~V2, newdata, lwd=2)
newdata
newdata <- matrix(seq(min(ytest), max(ytest),length=length(ytest)))
newdata=sort(cbind(newdata,xtest[,2]))
newdata <- matrix(seq(min(ytest), max(ytest),length=length(ytest)))
newdata
ytest
newdata <- matrix(final_test_pred)
newdata=sort(cbind(newdata,xtest[,2]))
plot(V1~V2, data=newdata, col="steelblue")
newdata <- matrix(final_test_pred)
newdata=sort(rbind(newdata,xtest[,2]))
final_test_pred
newdata <- matrix(t(final_test_pred))
View(newdata)
newdata=sort(bind(newdata,xtest[,2]))
newdata=sort(cbind(newdata,xtest[,2]))
plot(V1~V2, data=newdata, col="steelblue")
lines(V1~V2, newdata, lwd=2)
